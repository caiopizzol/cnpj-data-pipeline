# Production configuration - assumes external database
# Usage: docker-compose -f docker-compose.prod.yml up

services:
  pipeline:
    # image: cnpj-pipeline:${VERSION:-latest}
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - VERSION=${VERSION:-latest}
    restart: "no"  # Batch job - don't restart
    volumes:
      - pipeline_temp:/app/temp
      - pipeline_logs:/app/logs
    environment:
      # All configuration from environment/secrets
      DATABASE_BACKEND: ${DATABASE_BACKEND}

      # PostgreSQL config
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}

      # OR MySQL config
      # MYSQL_HOST: ${MYSQL_HOST}
      # MYSQL_PORT: ${MYSQL_PORT}
      # MYSQL_DATABASE: ${MYSQL_DATABASE}
      # MYSQL_USER: ${MYSQL_USER}
      # MYSQL_PASSWORD: ${MYSQL_PASSWORD}

      # OR BigQuery config
      # BQ_PROJECT_ID: ${BQ_PROJECT_ID}
      # BQ_DATASET: ${BQ_DATASET}
      # GOOGLE_APPLICATION_CREDENTIALS: ${GOOGLE_APPLICATION_CREDENTIALS}

      # Processing config
      PROCESSING_STRATEGY: ${PROCESSING_STRATEGY:-auto}
      BATCH_SIZE: ${BATCH_SIZE:-100000}
      MAX_MEMORY_PERCENT: ${MAX_MEMORY_PERCENT:-75}
      DEBUG: ${DEBUG:-false}

      # Download config
      DOWNLOAD_STRATEGY: ${DOWNLOAD_STRATEGY:-sequential}
      DOWNLOAD_WORKERS: ${DOWNLOAD_WORKERS:-4}
      KEEP_DOWNLOADED_FILES: ${KEEP_DOWNLOADED_FILES:-false}

volumes:
  pipeline_temp:
  pipeline_logs:
